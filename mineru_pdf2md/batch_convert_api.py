"""
æ‰¹é‡PDFè½¬Markdownå¤„ç†è„šæœ¬
ä½¿ç”¨MinerU APIå¤„ç†pdfsæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶

è¾“å‡ºç»“æ„:
    outputs_api/
    â”œâ”€â”€ DSS/
    â”‚   â””â”€â”€ æ–‡ä»¶å1/
    â”‚       â”œâ”€â”€ æ–‡ä»¶å1.md
    â”‚       â””â”€â”€ images/
    â”œâ”€â”€ EJIS/
    â”‚   â””â”€â”€ æ–‡ä»¶å2/
    â”‚       â””â”€â”€ ...
    â””â”€â”€ ...

ä½¿ç”¨æ–¹æ³•:
    python batch_convert_api.py
    python batch_convert_api.py --input-dir pdfs --output-dir outputs_api
    python batch_convert_api.py --language en --no-ocr
"""

import argparse
import asyncio
import json
import re
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

from mineru_api_base import (
    MinerUAPIClient,
    BaseBatchProcessor,
    TaskState,
    TaskResult,
    logger,
)


def sanitize_filename(name: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œä½¿å…¶ç¬¦åˆWindowsç³»ç»Ÿçš„è·¯å¾„è¦æ±‚
    """
    # æ›¿æ¢Windowsä¸æ”¯æŒçš„å­—ç¬¦: \ / : * ? " < > |
    invalid_chars = r'[\\/:*?"<>|]'
    clean_name = re.sub(invalid_chars, '_', name)
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œæœ«å°¾çš„ç‚¹
    return clean_name.strip().rstrip('.')


class PDFBatchProcessor(BaseBatchProcessor):
    """
    PDFæ‰¹é‡å¤„ç†å™¨
    
    å¤„ç†pdfsæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å­æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶ï¼Œ
    ä¿æŒç›¸åŒçš„ç›®å½•ç»“æ„è¾“å‡ºåˆ°outputs_apiæ–‡ä»¶å¤¹ã€‚
    """
    
    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS = {'.pdf', '.PDF'}
    
    def __init__(
        self,
        input_dir: str = "pdfs",
        output_dir: str = "outputs_api",
        client: Optional[MinerUAPIClient] = None,
        **client_kwargs
    ):
        """
        åˆå§‹åŒ–PDFæ‰¹é‡å¤„ç†å™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•ï¼ˆåŒ…å«PDFæ–‡ä»¶çš„æ ¹ç›®å½•ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            client: MinerUAPIClientå®ä¾‹
            **client_kwargs: ä¼ é€’ç»™MinerUAPIClientçš„å‚æ•°
        """
        super().__init__(client, **client_kwargs)
        
        self.input_dir = Path(input_dir).resolve()
        self.output_dir = Path(output_dir).resolve()
        
        if not self.input_dir.exists():
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"PDFæ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è¾“å…¥ç›®å½•: {self.input_dir}")
        logger.info(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def find_files(self) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æ‰€æœ‰éœ€è¦å¤„ç†çš„PDFæ–‡ä»¶
        
        Returns:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«:
            - path: æ–‡ä»¶å®Œæ•´è·¯å¾„
            - output_dir: å¯¹åº”çš„è¾“å‡ºç›®å½•
            - subfolder: ç›¸å¯¹äºinput_dirçš„å­æ–‡ä»¶å¤¹è·¯å¾„
            - filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        files = []
        
        try:
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
            for pdf_path in self.input_dir.rglob("*"):
                # è·³è¿‡éæ–‡ä»¶
                if not pdf_path.is_file():
                    continue
                
                # æ£€æŸ¥æ‰©å±•å
                if pdf_path.suffix not in self.SUPPORTED_EXTENSIONS:
                    continue
                
                # è·³è¿‡zipæ–‡ä»¶å†…çš„å†…å®¹
                if '.zip' in str(pdf_path):
                    continue
                
                try:
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„
                    relative_path = pdf_path.relative_to(self.input_dir)
                    
                    # æ¸…ç†è·¯å¾„ä¸­çš„æ¯ä¸€éƒ¨åˆ†ï¼ˆå¤„ç†éæ³•å­—ç¬¦å’Œç»“å°¾ç©ºæ ¼/ç‚¹ï¼‰
                    # æ¯”å¦‚ 'EJIS /file .pdf' -> 'EJIS/file'
                    clean_parts = [sanitize_filename(p) for p in relative_path.parent.parts]
                    clean_subfolder = Path(*clean_parts) if clean_parts else Path('.')
                    filename = sanitize_filename(pdf_path.stem)
                    
                    # è®¡ç®—è¾“å‡ºç›®å½•: outputs_api/å­æ–‡ä»¶å¤¹/æ–‡ä»¶å/
                    file_output_dir = self.output_dir / clean_subfolder / filename
                    
                    files.append({
                        'path': pdf_path,
                        'output_dir': file_output_dir,
                        'subfolder': clean_subfolder,
                        'filename': filename,
                    })
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ä»¶è·¯å¾„æ—¶å‡ºé”™ {pdf_path}: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªPDFæ–‡ä»¶")
        return files
    
    def is_processed(self, file_info: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†
        
        åˆ¤æ–­ä¾æ®ï¼šè¾“å‡ºç›®å½•ä¸‹å­˜åœ¨.mdæ–‡ä»¶
        """
        try:
            output_dir = Path(file_info['output_dir'])
            
            if not output_dir.exists():
                return False
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨.mdæ–‡ä»¶
            md_files = list(output_dir.rglob("*.md"))
            if md_files:
                # æ£€æŸ¥mdæ–‡ä»¶æ˜¯å¦æœ‰å†…å®¹
                for md_file in md_files:
                    try:
                        if md_file.stat().st_size > 0:
                            return True
                    except:
                        continue
            
            return False
        
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å¤„ç†çŠ¶æ€æ—¶å‡ºé”™ {file_info['path']}: {e}")
            return False
    
    def on_file_success(self, file_info: Dict[str, Any], result: TaskResult) -> None:
        """æ–‡ä»¶å¤„ç†æˆåŠŸå›è°ƒ"""
        try:
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            output_dir = Path(file_info['output_dir'])
            images_dir = output_dir / "images"
            img_count = 0
            
            if images_dir.exists():
                img_count = len(list(images_dir.glob("*")))
            else:
                # å¯èƒ½åœ¨å­ç›®å½•ä¸­
                for subdir in output_dir.rglob("images"):
                    if subdir.is_dir():
                        img_count += len(list(subdir.glob("*")))
            
            logger.info(
                f"âœ… æˆåŠŸ: {file_info['filename']}.pdf "
                f"(å­ç›®å½•: {file_info['subfolder']}, å›¾ç‰‡: {img_count})"
            )
        except Exception as e:
            logger.info(f"âœ… æˆåŠŸ: {file_info['filename']}.pdf")
    
    def on_file_error(self, file_info: Dict[str, Any], error: Exception) -> None:
        """æ–‡ä»¶å¤„ç†å¤±è´¥å›è°ƒ"""
        logger.error(
            f"âŒ å¤±è´¥: {file_info['filename']}.pdf "
            f"(å­ç›®å½•: {file_info['subfolder']}) - {error}"
        )
    
    def cleanup_partial_output(self, file_info: Dict[str, Any]) -> None:
        """æ¸…ç†éƒ¨åˆ†å®Œæˆçš„è¾“å‡º"""
        try:
            output_dir = Path(file_info['output_dir'])
            if output_dir.exists():
                shutil.rmtree(output_dir)
                logger.debug(f"å·²æ¸…ç†éƒ¨åˆ†è¾“å‡º: {output_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†è¾“å‡ºå¤±è´¥ {file_info['output_dir']}: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å½“å‰å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        files = self.find_files()
        processed = sum(1 for f in files if self.is_processed(f))
        
        # æŒ‰å­æ–‡ä»¶å¤¹ç»Ÿè®¡
        by_subfolder = {}
        # æ£€æŸ¥è¾“å‡ºè·¯å¾„å†²çª (å¤šä¸ªæºæ–‡ä»¶æ˜ å°„åˆ°åŒä¸€ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹)
        output_map = {} # output_path -> list of source_paths
        
        for file_info in files:
            subfolder = str(file_info['subfolder'])
            if subfolder not in by_subfolder:
                by_subfolder[subfolder] = {'total': 0, 'processed': 0}
            by_subfolder[subfolder]['total'] += 1
            if self.is_processed(file_info):
                by_subfolder[subfolder]['processed'] += 1
            
            # è®°å½•è¾“å‡ºè·¯å¾„æ˜ å°„ç”¨äºå†²çªæ£€æµ‹
            out_path_str = str(file_info['output_dir'])
            if out_path_str not in output_map:
                output_map[out_path_str] = []
            output_map[out_path_str].append(file_info['path'])
            
        # ç­›é€‰å‡ºæœ‰å†²çªçš„æ¡ç›®
        collisions = {k: v for k, v in output_map.items() if len(v) > 1}
        
        return {
            'total': len(files),
            'processed': processed,
            'remaining': len(files) - processed,
            'by_subfolder': by_subfolder,
            'collisions': collisions,
        }
    
    def process_all_sync(
        self,
        enable_ocr: bool = True,
        language: str = "ch",
        skip_processed: bool = True,
        batch_size: int = 1,
        delay_between_batches: float = 1.0,
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼ˆå¸¦æ‰¹æ¬¡æ§åˆ¶ï¼‰
        
        Args:
            enable_ocr: æ˜¯å¦å¯ç”¨OCR
            language: æ–‡æ¡£è¯­è¨€
            skip_processed: æ˜¯å¦è·³è¿‡å·²å¤„ç†æ–‡ä»¶
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°ï¼ˆç›®å‰APIé™åˆ¶ä¸º1ï¼‰
            delay_between_batches: æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿ(ç§’)
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        files = self.find_files()
        
        stats = {
            "total": len(files),
            "skipped": 0,
            "success": 0,
            "failed": 0,
            "errors": [],
            "start_time": datetime.now().isoformat(),
            "end_time": None,
        }
        
        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        files_to_process = []
        for file_info in files:
            try:
                if skip_processed and self.is_processed(file_info):
                    stats["skipped"] += 1
                    logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†: {file_info['subfolder']}/{file_info['filename']}.pdf")
                    continue
                files_to_process.append(file_info)
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ–‡ä»¶çŠ¶æ€æ—¶å‡ºé”™: {e}")
                files_to_process.append(file_info)
        
        logger.info(f"éœ€è¦å¤„ç†: {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        # é€ä¸ªå¤„ç†ï¼ˆAPIå½“å‰é™åˆ¶ï¼‰
        for idx, file_info in enumerate(files_to_process):
            try:
                logger.info(f"ğŸ“„ å¤„ç† [{idx+1}/{len(files_to_process)}]: "
                           f"{file_info['subfolder']}/{file_info['filename']}.pdf")
                
                # æ¸…ç†å¯èƒ½å­˜åœ¨çš„éƒ¨åˆ†è¾“å‡º
                self.cleanup_partial_output(file_info)
                
                # å¤„ç†æ–‡ä»¶
                task_info = self.client.process_file_sync(
                    str(file_info['path']),
                    str(file_info['output_dir']),
                    enable_ocr=enable_ocr,
                    language=language,
                )
                
                # æ£€æŸ¥ç»“æœ
                for result in task_info.results:
                    if result.status == TaskState.DONE:
                        stats["success"] += 1
                        self.on_file_success(file_info, result)
                    else:
                        stats["failed"] += 1
                        error_msg = result.error_message or "æœªçŸ¥é”™è¯¯"
                        self.on_file_error(file_info, Exception(error_msg))
                        stats["errors"].append({
                            "file": str(file_info['path']),
                            "subfolder": str(file_info['subfolder']),
                            "error": error_msg,
                        })
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if idx < len(files_to_process) - 1 and delay_between_batches > 0:
                    time.sleep(delay_between_batches)
                    
            except Exception as e:
                stats["failed"] += 1
                self.on_file_error(file_info, e)
                stats["errors"].append({
                    "file": str(file_info['path']),
                    "subfolder": str(file_info['subfolder']),
                    "error": str(e),
                })
                
                # æ¸…ç†å¤±è´¥çš„è¾“å‡º
                self.cleanup_partial_output(file_info)
        
        stats["end_time"] = datetime.now().isoformat()
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(stats)
        
        return stats
    
    async def process_all_async(
        self,
        enable_ocr: bool = True,
        language: str = "ch",
        skip_processed: bool = True,
        delay_between_batches: float = 1.0,
        batch_size: int = 1,
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
        
        Args:
            enable_ocr: æ˜¯å¦å¯ç”¨OCR
            language: æ–‡æ¡£è¯­è¨€
            skip_processed: æ˜¯å¦è·³è¿‡å·²å¤„ç†æ–‡ä»¶
            delay_between_batches: æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿ(ç§’)
            batch_size: å¹¶å‘å¤„ç†çš„æ–‡ä»¶æ•°é‡
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        files = self.find_files()
        
        stats = {
            "total": len(files),
            "skipped": 0,
            "success": 0,
            "failed": 0,
            "errors": [],
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "batch_size": batch_size,
        }
        
        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        files_to_process = []
        for file_info in files:
            try:
                if skip_processed and self.is_processed(file_info):
                    stats["skipped"] += 1
                    logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†: {file_info['subfolder']}/{file_info['filename']}.pdf")
                    continue
                files_to_process.append(file_info)
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ–‡ä»¶çŠ¶æ€æ—¶å‡ºé”™: {e}")
                files_to_process.append(file_info)
        
        total_to_process = len(files_to_process)
        logger.info(f"éœ€è¦å¤„ç†: {total_to_process} ä¸ªæ–‡ä»¶ï¼Œå¹¶å‘æ•°: {batch_size}")
        
        # æŒ‰batch_sizeåˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_to_process, batch_size):
            batch_end = min(batch_start + batch_size, total_to_process)
            batch_files = files_to_process[batch_start:batch_end]
            batch_num = batch_start // batch_size + 1
            total_batches = (total_to_process + batch_size - 1) // batch_size
            
            logger.info(f"ğŸš€ æ‰¹æ¬¡ [{batch_num}/{total_batches}]: å¹¶å‘å¤„ç† {len(batch_files)} ä¸ªæ–‡ä»¶")
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = []
            for file_info in batch_files:
                task = self._process_single_file_async(
                    file_info, enable_ocr, language, stats
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_end < total_to_process and delay_between_batches > 0:
                logger.info(f"â³ ç­‰å¾… {delay_between_batches} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
                await asyncio.sleep(delay_between_batches)
        
        stats["end_time"] = datetime.now().isoformat()
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(stats)
        
        return stats
    
    async def _process_single_file_async(
        self,
        file_info: Dict[str, Any],
        enable_ocr: bool,
        language: str,
        stats: Dict[str, Any],
    ) -> None:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯
            enable_ocr: æ˜¯å¦å¯ç”¨OCR
            language: æ–‡æ¡£è¯­è¨€
            stats: ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼šåŸåœ°ä¿®æ”¹ï¼‰
        """
        try:
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†: {file_info['subfolder']}/{file_info['filename']}.pdf")
            
            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„éƒ¨åˆ†è¾“å‡º
            self.cleanup_partial_output(file_info)
            
            # å¤„ç†æ–‡ä»¶
            task_info = await self.client.process_file(
                str(file_info['path']),
                str(file_info['output_dir']),
                enable_ocr=enable_ocr,
                language=language,
            )
            
            # æ£€æŸ¥ç»“æœ
            for result in task_info.results:
                if result.status == TaskState.DONE:
                    stats["success"] += 1
                    self.on_file_success(file_info, result)
                else:
                    stats["failed"] += 1
                    error_msg = result.error_message or "æœªçŸ¥é”™è¯¯"
                    self.on_file_error(file_info, Exception(error_msg))
                    stats["errors"].append({
                        "file": str(file_info['path']),
                        "subfolder": str(file_info['subfolder']),
                        "error": error_msg,
                    })
                    
        except Exception as e:
            stats["failed"] += 1
            self.on_file_error(file_info, e)
            stats["errors"].append({
                "file": str(file_info['path']),
                "subfolder": str(file_info['subfolder']),
                "error": str(e),
            })
            
            # æ¸…ç†å¤±è´¥çš„è¾“å‡º
            self.cleanup_partial_output(file_info)
    
    def _print_summary(self, stats: Dict[str, Any]) -> None:
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        logger.info("=" * 60)
        logger.info("å¤„ç†å®Œæˆæ‘˜è¦")
        logger.info("=" * 60)
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {stats['total']}")
        logger.info(f"  è·³è¿‡(å·²å¤„ç†): {stats['skipped']}")
        logger.info(f"  æˆåŠŸ: {stats['success']}")
        logger.info(f"  å¤±è´¥: {stats['failed']}")
        logger.info(f"  å¼€å§‹æ—¶é—´: {stats['start_time']}")
        logger.info(f"  ç»“æŸæ—¶é—´: {stats['end_time']}")
        
        if stats['errors']:
            logger.info("-" * 60)
            logger.info("é”™è¯¯è¯¦æƒ…:")
            for error in stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                logger.info(f"  - {error['file']}: {error['error']}")
            if len(stats['errors']) > 10:
                logger.info(f"  ... è¿˜æœ‰ {len(stats['errors']) - 10} ä¸ªé”™è¯¯")
        
        logger.info("=" * 60)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨MinerU APIæ‰¹é‡å¤„ç†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdown",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                          # ä½¿ç”¨é»˜è®¤è®¾ç½®å¤„ç†
  %(prog)s --input-dir pdfs         # æŒ‡å®šè¾“å…¥ç›®å½•
  %(prog)s --output-dir outputs     # æŒ‡å®šè¾“å‡ºç›®å½•
  %(prog)s --language en            # è®¾ç½®æ–‡æ¡£è¯­è¨€ä¸ºè‹±æ–‡
  %(prog)s --no-ocr                 # ç¦ç”¨OCR
  %(prog)s --no-skip                # ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
  %(prog)s --stats                  # åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  %(prog)s --async                  # ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
        """
    )
    
    parser.add_argument(
        '--input-dir', '-i',
        type=str,
        default='pdfs',
        help='è¾“å…¥ç›®å½•è·¯å¾„ (é»˜è®¤: pdfs)'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='outputs_api',
        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: outputs_api)'
    )
    
    parser.add_argument(
        '--language', '-l',
        type=str,
        default='en',
        choices=['ch', 'en', 'korean', 'japan', 'chinese_cht', 'ta', 'te', 'ka', 'th', 'el', 'latin', 'arabic'],
        help='æ–‡æ¡£è¯­è¨€ (é»˜è®¤: en)'
    )
    
    parser.add_argument(
        '--no-ocr',
        action='store_true',
        help='ç¦ç”¨OCR'
    )
    
    parser.add_argument(
        '--no-skip',
        action='store_true',
        help='ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œä¸è¿›è¡Œå¤„ç†'
    )
    
    parser.add_argument(
        '--async',
        dest='use_async',
        action='store_true',
        help='ä½¿ç”¨å¼‚æ­¥æ¨¡å¼å¤„ç†ï¼ˆæ”¯æŒå¹¶å‘ï¼‰'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=1,
        help='å¹¶å‘å¤„ç†çš„æ–‡ä»¶æ•°é‡ (é»˜è®¤: 1, éœ€é…åˆ--asyncä½¿ç”¨)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=1.0,
        help='æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1.0)'
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        help='MinerU APIå¯†é’¥ (é»˜è®¤ä»token.txtæˆ–ç¯å¢ƒå˜é‡è¯»å–)'
    )
    
    parser.add_argument(
        '--max-retries',
        type=int,
        default=180,
        help='æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 180)'
    )
    
    parser.add_argument(
        '--retry-interval',
        type=int,
        default=10,
        help='é‡è¯•é—´éš”ç§’æ•° (é»˜è®¤: 10)'
    )
    
    return parser.parse_args()


def show_statistics(processor: PDFBatchProcessor) -> None:
    """æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    stats = processor.get_statistics()
    
    print("\n" + "=" * 60)
    print("PDFå¤„ç†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    
    # æ£€æŸ¥å†²çª
    collisions = stats.get('collisions', {})
    if collisions:
        print("\n" + "!" * 60)
        print(f"âš ï¸ è­¦å‘Š: å‘ç° {len(collisions)} ç»„å‘½åå†²çªï¼")
        print("ä»¥ä¸‹æ–‡ä»¶ä¼šè¢«æ˜ å°„åˆ°åŒä¸€ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹ï¼Œå¯èƒ½å¯¼è‡´è¦†ç›–æˆ–è·³è¿‡ï¼š")
        print("-" * 60)
        for out_dir, sources in collisions.items():
            print(f"è¾“å‡ºç›®å½•: {out_dir}")
            for src in sources:
                print(f"  - {src}")
            print("-" * 30)
        print("!" * 60 + "\n")
        
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total']}")
    print(f"  å·²å¤„ç†: {stats['processed']}")
    print(f"  å¾…å¤„ç†: {stats['remaining']}")
    print()
    print("æŒ‰æ–‡ä»¶å¤¹ç»Ÿè®¡:")
    print("-" * 60)
    
    for subfolder, counts in sorted(stats['by_subfolder'].items()):
        print(f"  {subfolder or '(æ ¹ç›®å½•)'}: "
              f"{counts['processed']}/{counts['total']} å·²å¤„ç†")
    
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    try:
        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_kwargs = {
            'max_retries': args.max_retries,
            'retry_interval': args.retry_interval,
        }
        if args.api_key:
            client_kwargs['api_key'] = args.api_key
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = PDFBatchProcessor(
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            **client_kwargs
        )
        
        # åªæ˜¾ç¤ºç»Ÿè®¡
        if args.stats:
            show_statistics(processor)
            return 0
        
        # å¤„ç†å‚æ•°
        process_kwargs = {
            'enable_ocr': not args.no_ocr,
            'language': args.language,
            'skip_processed': not args.no_skip,
        }
        
        # å¼€å§‹å¤„ç†
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†...")
        
        if args.use_async:
            # å¼‚æ­¥æ¨¡å¼ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
            process_kwargs['delay_between_batches'] = args.delay
            process_kwargs['batch_size'] = args.batch_size
            stats = asyncio.run(processor.process_all_async(**process_kwargs))
        else:
            # åŒæ­¥æ¨¡å¼
            process_kwargs['delay_between_batches'] = args.delay
            stats = processor.process_all_sync(**process_kwargs)
        
        # ä¿å­˜å¤„ç†ç»“æœ
        result_file = Path(args.output_dir) / 'processing_result.json'
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            logger.info(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
        
        # è¿”å›çŠ¶æ€ç 
        return 0 if stats['failed'] == 0 else 1
        
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 130
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
